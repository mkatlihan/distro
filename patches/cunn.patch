diff --git a/CMakeLists.txt b/CMakeLists.txt
index 501a2de..7e1f305 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -2,6 +2,7 @@ CMAKE_MINIMUM_REQUIRED(VERSION 2.8 FATAL_ERROR)
 CMAKE_POLICY(VERSION 2.8)
 
 SET(CMAKE_MODULE_PATH "${CMAKE_CURRENT_SOURCE_DIR}/cmake" "${CMAKE_MODULE_PATH}")
+add_definitions(-D_ALLOW_COMPILER_AND_STL_VERSION_MISMATCH)
 
 FIND_PACKAGE(Torch REQUIRED)
 FIND_PACKAGE(CUDA 6.5 REQUIRED)
diff --git a/lib/THCUNN/CMakeLists.txt b/lib/THCUNN/CMakeLists.txt
index 6bc5813..d47ff5e 100644
--- a/lib/THCUNN/CMakeLists.txt
+++ b/lib/THCUNN/CMakeLists.txt
@@ -37,9 +37,11 @@ IF ($ENV{TH_BINARY_BUILD})
     SET(CMAKE_CXX_FLAGS "-Wl,--exclude-libs,libstdc++.a ${CMAKE_CXX_FLAGS}")
   ENDIF(UNIX AND NOT APPLE)
 ENDIF()
-
+MESSAGE(STATUS "Detect CUDA architecture and get best NVCC flags")
+MESSAGE(STATUS "CUDA_SELECT_NVCC_ARCH_FLAGS: " ${CUDA_SELECT_NVCC_ARCH_FLAGS})
 #Â Detect CUDA architecture and get best NVCC flags
 IF(NOT COMMAND CUDA_SELECT_NVCC_ARCH_FLAGS OR MSVC)
+  MESSAGE(STATUS "Including ${CMAKE_CURRENT_SOURCE_DIR}/cmake/select_compute_arch.cmake")
   INCLUDE(${CMAKE_CURRENT_SOURCE_DIR}/cmake/select_compute_arch.cmake)
 ENDIF()
 LIST(APPEND CUDA_NVCC_FLAGS $ENV{TORCH_NVCC_FLAGS})
@@ -72,6 +74,8 @@ ENDIF()
 FILE(GLOB src-cuda *.cu)
 
 CUDA_INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR})
+MESSAGE(STATUS "got cuda version " ${CUDA_VERSION})
+MESSAGE(STATUS "CUDA_NVCC_FLAGS: ${CUDA_NVCC_FLAGS}")
 CUDA_ADD_LIBRARY(THCUNN MODULE ${src-cuda})
 
 IF(MSVC)
diff --git a/lib/THCUNN/LogSigmoid.cu b/lib/THCUNN/LogSigmoid.cu
index dbdfea5..1f9fa21 100644
--- a/lib/THCUNN/LogSigmoid.cu
+++ b/lib/THCUNN/LogSigmoid.cu
@@ -7,7 +7,7 @@ template <typename T>
 struct logSigmoid_updateOutput_functor
 {
   __device__ void operator()(T *output, const T *input) const {
-    const T max = fmaxType(0.f, - *input);
+    const T max = fmaxType(static_cast<T>(0), - *input);
     const T z = THCNumerics<T>::exp(-max) + THCNumerics<T>::exp(-*input -max);
     *output = -(max + THCNumerics<T>::log(z));
   }
@@ -17,7 +17,7 @@ template <typename T>
 struct logSigmoid_updateGradInput_functor
 {
   __device__ void operator()(T *gradInput, const T *input, const T *gradOutput) const {
-    const T max = fmaxType(0.f, -*input);
+    const T max = fmaxType(static_cast<T>(0), -*input);
     const T z = THCNumerics<T>::exp(-max) + THCNumerics<T>::exp(-*input -max);
     T max_deriv = 0.f;
     T sign = -1.f;
diff --git a/lib/THCUNN/LookupTable.cu b/lib/THCUNN/LookupTable.cu
index 116639b..cdb4a6d 100644
--- a/lib/THCUNN/LookupTable.cu
+++ b/lib/THCUNN/LookupTable.cu
@@ -29,7 +29,8 @@ __device__ __forceinline__ bool warpHasCollision(int val)
   #pragma unroll
   for (int i = 1; i <= 16; i++)
   {
-    dup |= (__shfl(val, (laneId + i) % 32) == val);
+    //dup |= (__shfl(val, (laneId + i) % 32) == val);
+    dup |= (__shfl_sync(0xFFFFFFFF, val, (laneId + i) % 32) == val);
   }
 
 #else
@@ -46,7 +47,8 @@ __device__ __forceinline__ bool warpHasCollision(int val)
 
 #endif
 
-  return __any(dup) != 0;
+  //return __any(dup) != 0;
+  return __any_sync(0xFFFFFFFF, dup) != 0;
 }
 
 template <typename Dtype>
diff --git a/lib/THCUNN/cmake/select_compute_arch.cmake b/lib/THCUNN/cmake/select_compute_arch.cmake
index bff85de..da2111b 100644
--- a/lib/THCUNN/cmake/select_compute_arch.cmake
+++ b/lib/THCUNN/cmake/select_compute_arch.cmake
@@ -21,7 +21,8 @@
 set(CUDA_KNOWN_GPU_ARCHITECTURES  "Fermi" "Kepler" "Maxwell")
 
 # This list will be used for CUDA_ARCH_NAME = Common option (enabled by default)
-set(CUDA_COMMON_GPU_ARCHITECTURES "3.0" "3.5" "5.0")
+#set(CUDA_COMMON_GPU_ARCHITECTURES "3.0" "3.5" "5.0")
+set(CUDA_COMMON_GPU_ARCHITECTURES "3.5" "5.0")
 
 if (CUDA_VERSION VERSION_GREATER "6.5")
   list(APPEND CUDA_KNOWN_GPU_ARCHITECTURES "Kepler+Tegra" "Kepler+Tesla" "Maxwell+Tegra")
@@ -31,9 +32,13 @@ endif ()
 if (CUDA_VERSION VERSION_GREATER "7.5")
   list(APPEND CUDA_KNOWN_GPU_ARCHITECTURES "Pascal")
   list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "6.0" "6.1" "6.1+PTX")
+  list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "8.9")
 else()
   list(APPEND CUDA_COMMON_GPU_ARCHITECTURES "5.2+PTX")
 endif ()
+set(CUDA_NVCC_FLAGS "${CUDA_NVCC_FLAGS} -allow-unsupported-compiler")
+# Append C++11 to CUDA NVCC flags
+#list(APPEND CUDA_NVCC_FLAGS -std=c++11)
 
 
 
@@ -70,15 +75,16 @@ function(CUDA_DETECT_INSTALLED_GPUS OUT_VARIABLE)
 
     if(nvcc_res EQUAL 0)
       # only keep the last line of nvcc_out
-      string(REGEX REPLACE ";" "\\\\;" nvcc_out "${nvcc_out}")
-      string(REGEX REPLACE "\n" ";" nvcc_out "${nvcc_out}")
+      STRING(REGEX REPLACE ";" "\\\\;" nvcc_out "${nvcc_out}")
+      STRING(REGEX REPLACE "\n" ";" nvcc_out "${nvcc_out}")
       list(GET nvcc_out -1 nvcc_out)
       string(REPLACE "2.1" "2.1(2.0)" nvcc_out "${nvcc_out}")
       set(CUDA_GPU_DETECT_OUTPUT ${nvcc_out} CACHE INTERNAL "Returned GPU architetures from detect_gpus tool" FORCE)
     endif()
   endif()
 
-  if(NOT CUDA_GPU_DETECT_OUTPUT)
+#  if(NOT CUDA_GPU_DETECT_OUTPUT)
+  if(TRUE)
     message(STATUS "Automatic GPU detection failed. Building for common architectures.")
     set(${OUT_VARIABLE} ${CUDA_COMMON_GPU_ARCHITECTURES} PARENT_SCOPE)
   else()
diff --git a/lib/THCUNN/generic/SparseLinear.cu b/lib/THCUNN/generic/SparseLinear.cu
index dc4c6d4..08ca25e 100644
--- a/lib/THCUNN/generic/SparseLinear.cu
+++ b/lib/THCUNN/generic/SparseLinear.cu
@@ -28,205 +28,298 @@ static inline void copyCudaFloatingType(THCState *state, THCudaIntTensor *buf, T
 }
 
 void THNN_(SparseLinear_updateOutput)(
-           THCState *state,
-           THCTensor *input,
-           THCTensor *output,
-           THCTensor *weight,
-           THCTensor *bias)
+    THCState *state,
+    THCTensor *input,
+    THCTensor *output,
+    THCTensor *weight,
+    THCTensor *bias)
 {
-  THAssert(THCTensor_(checkGPU)(state, 4, input, output, weight, bias));
-
-  long h;
-  long outDim = THCTensor_(size)(state, weight, 0);
-  long inDim = THCTensor_(size)(state, weight, 1);
-
-  THArgCheck(checkInput(input), 2, "input size must be nnz x 3");
-  THArgCheck(THCTensor_(nDimension)(state, output) == 2, 3, "output must be batchsize x outputsize");
-  THArgCheck(checkSize1D(bias, outDim), 5, "bias size wrong");
-
-  weight = THCTensor_(newContiguous)(state, weight);
-  
-  long batchnum = THCTensor_(size)(state, output, 0);
-  long nnz = THCTensor_(size)(state, input, 0);
-
-  THCTensor *buffer = THCTensor_(new)(state);
-  THCTensor *sel = THCTensor_(new)(state);
-  THCTensor *values = THCTensor_(new)(state);
-  THCudaIntTensor *rowbuf = THCudaIntTensor_new(state);
-  THCudaIntTensor *csrPtrs = THCudaIntTensor_new(state);
-  THCudaIntTensor *colInds = THCudaIntTensor_new(state);
-
-  THCTensor_(resize1d)(state, values, nnz);
-  THCudaIntTensor_resize1d(state, rowbuf, nnz);
-  THCudaIntTensor_resize1d(state, colInds, nnz);
-  THCudaIntTensor_resize1d(state, csrPtrs, batchnum+1);
-
-  // Get data ready for cusparse, need CudaInt buffers
-  // We do not need to sort, since rows are already in order
-  // If rows might get out of order in future implementations, or if cusparse
-  //    complains with an illegal memory access, sort like we do in AccGradParameters
-  THCTensor_(select)(state, sel, input, 1, 0);
-  copyCudaFloatingType(state, rowbuf, sel);
-  THCTensor_(select)(state, sel, input, 1, 1);
-  copyCudaFloatingType(state, colInds, sel);
-  THCTensor_(select)(state, sel, input, 1, 2);
-  THCTensor_(copyCuda)(state, values, sel);
-
-  init_cusparse();
-  cusparseXcoo2csr(cusparse_handle,
-      THCudaIntTensor_data(state, rowbuf), nnz, batchnum,
-      THCudaIntTensor_data(state, csrPtrs), CUSPARSE_INDEX_BASE_ONE);
-
-  // output = bias
-  THCTensor_(resize2d)(state, buffer, outDim, batchnum);
-  THCTensor_(zero)(state, buffer);
-  for (h=0; h<batchnum; h++) {
-    THCTensor_(select)(state, sel, buffer, 1, h);
-    THCTensor_(copy)(state, sel, bias);
-  }
-
-  // output = W * x
-  real one = ScalarConvert<int, real>::to(1);
-  cusparseMatDescr_t descr = 0;
-  cusparseCreateMatDescr(&descr);
-  cusparseSetMatType(descr,CUSPARSE_MATRIX_TYPE_GENERAL);
-  cusparseSetMatIndexBase(descr,CUSPARSE_INDEX_BASE_ONE);
-  #ifdef THC_REAL_IS_FLOAT
-  cusparseScsrmm(cusparse_handle,
-  #elif defined(THC_REAL_IS_DOUBLE)
-  cusparseDcsrmm(cusparse_handle,
-  #endif
-      CUSPARSE_OPERATION_NON_TRANSPOSE,
-      batchnum, outDim, inDim, nnz,
-      &one,
-      descr,
-      THCTensor_(data)(state, values),
-      THCudaIntTensor_data(state, csrPtrs),
-      THCudaIntTensor_data(state, colInds),
-      THCTensor_(data)(state, weight), inDim,
-      &one, THCTensor_(data)(state, buffer), batchnum
-  );
-  THCTensor_(transpose)(state, buffer, NULL, 0, 1);
-
-  // We do work in the buffer to keep the output contiguous
-  THCTensor_(copy)(state, output, buffer);
-
-  cusparseDestroyMatDescr(descr);
-  descr = 0;
-  THCTensor_(free)(state, buffer);
-  THCTensor_(free)(state, sel);
-  THCTensor_(free)(state, values);
-  THCTensor_(free)(state, weight);
-  THCudaIntTensor_free(state, rowbuf);
-  THCudaIntTensor_free(state, colInds);
-  THCudaIntTensor_free(state, csrPtrs);
+    THAssert(THCTensor_(checkGPU)(state, 4, input, output, weight, bias));
+
+    long h;
+    long outDim = THCTensor_(size)(state, weight, 0);
+    long inDim = THCTensor_(size)(state, weight, 1);
+
+    THArgCheck(checkInput(input), 2, "input size must be nnz x 3");
+    THArgCheck(THCTensor_(nDimension)(state, output) == 2, 3, "output must be batchsize x outputsize");
+    THArgCheck(checkSize1D(bias, outDim), 5, "bias size wrong");
+
+    weight = THCTensor_(newContiguous)(state, weight);
+    
+    long batchnum = THCTensor_(size)(state, output, 0);
+    long nnz = THCTensor_(size)(state, input, 0);
+
+    THCTensor *buffer = THCTensor_(new)(state);
+    THCTensor *sel = THCTensor_(new)(state);
+    THCTensor *values = THCTensor_(new)(state);
+    THCudaIntTensor *rowbuf = THCudaIntTensor_new(state);
+    THCudaIntTensor *csrPtrs = THCudaIntTensor_new(state);
+    THCudaIntTensor *colInds = THCudaIntTensor_new(state);
+
+    THCTensor_(resize1d)(state, values, nnz);
+    THCudaIntTensor_resize1d(state, rowbuf, nnz);
+    THCudaIntTensor_resize1d(state, colInds, nnz);
+    THCudaIntTensor_resize1d(state, csrPtrs, batchnum + 1);
+
+    // Prepare data for cusparse
+    THCTensor_(select)(state, sel, input, 1, 0);
+    copyCudaFloatingType(state, rowbuf, sel);
+    THCTensor_(select)(state, sel, input, 1, 1);
+    copyCudaFloatingType(state, colInds, sel);
+    THCTensor_(select)(state, sel, input, 1, 2);
+    THCTensor_(copyCuda)(state, values, sel);
+
+    init_cusparse();
+    cusparseXcoo2csr(cusparse_handle,
+                      THCudaIntTensor_data(state, rowbuf), nnz, batchnum,
+                      THCudaIntTensor_data(state, csrPtrs), CUSPARSE_INDEX_BASE_ONE);
+
+    // output = bias
+    THCTensor_(resize2d)(state, buffer, outDim, batchnum);
+    THCTensor_(zero)(state, buffer);
+    for (h = 0; h < batchnum; h++) {
+        THCTensor_(select)(state, sel, buffer, 1, h);
+        THCTensor_(copy)(state, sel, bias);
+    }
+
+    // output = W * x
+    real one = ScalarConvert<int, real>::to(1);
+    cusparseMatDescr_t descr = 0;
+    cusparseCreateMatDescr(&descr);
+    cusparseSetMatType(descr, CUSPARSE_MATRIX_TYPE_GENERAL);
+    cusparseSetMatIndexBase(descr, CUSPARSE_INDEX_BASE_ONE);
+
+    cusparseSpMatDescr_t matA;
+    cusparseDnMatDescr_t matB, matC;
+
+    // Create sparse matrix A
+#ifdef THC_REAL_IS_FLOAT
+    cusparseCreateCsr(&matA, outDim, inDim, nnz,
+                      THCudaIntTensor_data(state, csrPtrs),
+                      THCudaIntTensor_data(state, colInds),
+                      THCTensor_(data)(state, values),
+                      CUSPARSE_INDEX_32I, CUSPARSE_INDEX_32I,
+                      CUSPARSE_INDEX_BASE_ONE, CUDA_R_32F);
+
+    // Create dense matrix B (input) and C (output)
+    cusparseCreateDnMat(&matB, inDim, batchnum, inDim, THCTensor_(data)(state, weight), CUDA_R_32F, CUSPARSE_ORDER_COL);
+    cusparseCreateDnMat(&matC, outDim, batchnum, outDim, THCTensor_(data)(state, buffer), CUDA_R_32F, CUSPARSE_ORDER_COL);
+
+    // Perform matrix multiplication (SpMM)
+    cusparseSpMM(cusparse_handle,
+                 CUSPARSE_OPERATION_NON_TRANSPOSE,
+                 CUSPARSE_OPERATION_NON_TRANSPOSE,
+                 &one,
+                 matA,
+                 matB,
+                 &one,
+                 matC,
+                 CUDA_R_32F,
+                 CUSPARSE_SPMM_ALG_DEFAULT,
+                 NULL); // Ensure dBuffer is allocated properly
+
+#elif defined(THC_REAL_IS_DOUBLE)
+    cusparseCreateCsr(&matA, outDim, inDim, nnz,
+                      THCudaIntTensor_data(state, csrPtrs),
+                      THCudaIntTensor_data(state, colInds),
+                      THCTensor_(data)(state, values),
+                      CUSPARSE_INDEX_32I, CUSPARSE_INDEX_32I,
+                      CUSPARSE_INDEX_BASE_ONE, CUDA_R_64F);
+
+    // Create dense matrix B (input) and C (output)
+    cusparseCreateDnMat(&matB, inDim, batchnum, inDim, THCTensor_(data)(state, weight), CUDA_R_64F, CUSPARSE_ORDER_COL);
+    cusparseCreateDnMat(&matC, outDim, batchnum, outDim, THCTensor_(data)(state, buffer), CUDA_R_64F, CUSPARSE_ORDER_COL);
+
+    // Perform matrix multiplication (SpMM)
+    cusparseSpMM(cusparse_handle,
+                 CUSPARSE_OPERATION_NON_TRANSPOSE,
+                 CUSPARSE_OPERATION_NON_TRANSPOSE,
+                 &one,
+                 matA,
+                 matB,
+                 &one,
+                 matC,
+                 CUDA_R_64F,
+                 CUSPARSE_SPMM_ALG_DEFAULT,
+                 NULL); // Ensure dBuffer is allocated properly
+#endif
+
+    // Transpose the result if necessary
+    THCTensor_(transpose)(state, buffer, NULL, 0, 1);
+
+    // Copy result to output
+    THCTensor_(copy)(state, output, buffer);
+
+    // Clean up
+    cusparseDestroySpMat(matA);
+    cusparseDestroyDnMat(matB);
+    cusparseDestroyDnMat(matC);
+    cusparseDestroyMatDescr(descr);
+    descr = 0;
+    THCTensor_(free)(state, buffer);
+    THCTensor_(free)(state, sel);
+    THCTensor_(free)(state, values);
+    THCTensor_(free)(state, weight);
+    THCudaIntTensor_free(state, rowbuf);
+    THCudaIntTensor_free(state, colInds);
+    THCudaIntTensor_free(state, csrPtrs);
 }
 
+
 void THNN_(SparseLinear_accGradParameters)(
-           THCState *state,
-           THCTensor *input,
-           THCTensor *gradOutput,
-           THCTensor *gradWeight,
-           THCTensor *gradBias,
-           THCTensor *weight,
-           THCTensor *bias,
-           accreal weightDecay,
-           accreal scale)
+    THCState *state,
+    THCTensor *input,
+    THCTensor *gradOutput,
+    THCTensor *gradWeight,
+    THCTensor *gradBias,
+    THCTensor *weight,
+    THCTensor *bias,
+    accreal weightDecay,
+    accreal scale)
 {
-  long outDim = THCTensor_(size)(state, weight, 0);
-  long inDim = THCTensor_(size)(state, weight, 1);
-
-  THArgCheck(checkInput(input), 2, "input size must be batchsize x nnz x 2");
-  THArgCheck(checkSize2D(gradWeight, outDim, inDim), 4, "gradWeight size wrong");
-  THArgCheck(checkSize1D(gradBias, outDim), 5, "gradBias size wrong");
-
-  weight = THCTensor_(newContiguous)(state, weight);
-  long nnz = THCTensor_(size)(state, input, 0);
-  long batchnum = THCTensor_(size)(state, gradOutput, 0);
-
-  THCTensor *buf = THCTensor_(new)(state);
-  THCTensor *cols = THCTensor_(new)(state);
-  THCTensor *sel = THCTensor_(new)(state);
-  THCudaLongTensor *inds = THCudaLongTensor_new(state);
-  THCTensor *values = THCTensor_(new)(state);
-  THCudaIntTensor *colbuf = THCudaIntTensor_new(state);
-  THCudaIntTensor *colPtrs = THCudaIntTensor_new(state);
-  THCudaIntTensor *rowInds = THCudaIntTensor_new(state);
-
-  THCTensor_(select)(state, sel, input, 1, 0); // rowInds
-  THCTensor_(select)(state, cols, input, 1, 1); // colInds
-  THCTensor_(cadd)(state, buf, sel, batchnum, cols); // colInds * buatchdim + rowInds
-  THCTensor_(sort)(state, buf, inds, buf, 0, 0); // Indices are now in ind
-  THCTensor_(indexSelect)(state, buf, input, 0, inds);
-
-  THCTensor_(resize1d)(state, values, nnz);
-  THCudaIntTensor_resize1d(state, colbuf, nnz);
-  THCudaIntTensor_resize1d(state, rowInds, nnz);
-  THCudaIntTensor_resize1d(state, colPtrs, inDim+1);
-
-  // Get data ready for cusparse, need CudaInt buffers
-  THCTensor_(select)(state, sel, buf, 1, 0);
-  copyCudaFloatingType(state, rowInds, sel);
-  THCTensor_(select)(state, sel, buf, 1, 1);
-  copyCudaFloatingType(state, colbuf, sel);
-  THCTensor_(select)(state, sel, buf, 1, 2);
-  THCTensor_(copyCuda)(state, values, sel);
-
-  init_cusparse();
-  // Secretly coo2csc
-  cusparseXcoo2csr(cusparse_handle,
-      THCudaIntTensor_data(state, colbuf), nnz, inDim,
-      THCudaIntTensor_data(state, colPtrs), CUSPARSE_INDEX_BASE_ONE);
-
-  // FORTRAN expects contiguous col-major matricies
-  THCTensor *tgradOutput = THCTensor_(new)(state);
-  THCTensor_(transpose)(state, tgradOutput, gradOutput, 0, 1);
-  THCTensor_(resize2d)(state, buf, batchnum, outDim);
-  THCTensor_(copy)(state, buf, tgradOutput);
-  THCTensor_(free)(state, tgradOutput);
-
-  real one = ScalarConvert<int, real>::to(1);
-  cusparseMatDescr_t descr = 0;
-  cusparseCreateMatDescr(&descr);
-  cusparseSetMatType(descr,CUSPARSE_MATRIX_TYPE_GENERAL);
-  cusparseSetMatIndexBase(descr,CUSPARSE_INDEX_BASE_ONE);
-  #ifdef THC_REAL_IS_FLOAT
-  cusparseScsrmm(cusparse_handle,
-  #elif defined(THC_REAL_IS_DOUBLE)
-  cusparseDcsrmm(cusparse_handle,
-  #endif
-      CUSPARSE_OPERATION_NON_TRANSPOSE,
-      inDim, outDim, batchnum, nnz,
-      &one,
-      descr,
-      THCTensor_(data)(state, values),
-      THCudaIntTensor_data(state, colPtrs),
-      THCudaIntTensor_data(state, rowInds),
-      THCTensor_(data)(state, buf), batchnum,
-      &one, THCTensor_(data)(state, gradWeight), inDim
-  );
-
-  THCTensor_(sum)(state, buf, gradOutput, 0, 1);
-  THCTensor_(resize1d)(state, buf, outDim);
-  THCTensor_(cadd)(state, gradBias, gradBias, scale, buf);
-
-  if (weightDecay != 0)
-  {
-    THCTensor_(cadd)(state, gradWeight, gradWeight, weightDecay, weight);
-    THCTensor_(cadd)(state, gradBias, gradBias, weightDecay, bias);
-  }
-
-  THCTensor_(free)(state, weight);
-  THCTensor_(free)(state, buf);
-  THCTensor_(free)(state, sel);
-  THCTensor_(free)(state, cols);
-  THCudaLongTensor_free(state, inds);
-  THCTensor_(free)(state, values);
-  THCudaIntTensor_free(state, colbuf);
-  THCudaIntTensor_free(state, rowInds);
-  THCudaIntTensor_free(state, colPtrs);
+    long outDim = THCTensor_(size)(state, weight, 0);
+    long inDim = THCTensor_(size)(state, weight, 1);
+
+    THArgCheck(checkInput(input), 2, "input size must be batchsize x nnz x 2");
+    THArgCheck(checkSize2D(gradWeight, outDim, inDim), 4, "gradWeight size wrong");
+    THArgCheck(checkSize1D(gradBias, outDim), 5, "gradBias size wrong");
+
+    weight = THCTensor_(newContiguous)(state, weight);
+    long nnz = THCTensor_(size)(state, input, 0);
+    long batchnum = THCTensor_(size)(state, gradOutput, 0);
+
+    THCTensor *buf = THCTensor_(new)(state);
+    THCTensor *cols = THCTensor_(new)(state);
+    THCTensor *sel = THCTensor_(new)(state);
+    THCudaLongTensor *inds = THCudaLongTensor_new(state);
+    THCTensor *values = THCTensor_(new)(state);
+    THCudaIntTensor *colbuf = THCudaIntTensor_new(state);
+    THCudaIntTensor *colPtrs = THCudaIntTensor_new(state);
+    THCudaIntTensor *rowInds = THCudaIntTensor_new(state);
+
+    THCTensor_(select)(state, sel, input, 1, 0); // rowInds
+    THCTensor_(select)(state, cols, input, 1, 1); // colInds
+    THCTensor_(cadd)(state, buf, sel, batchnum, cols); // colInds * batchnum + rowInds
+    THCTensor_(sort)(state, buf, inds, buf, 0, 0); // Indices are now in inds
+    THCTensor_(indexSelect)(state, buf, input, 0, inds);
+
+    THCTensor_(resize1d)(state, values, nnz);
+    THCudaIntTensor_resize1d(state, colbuf, nnz);
+    THCudaIntTensor_resize1d(state, rowInds, nnz);
+    THCudaIntTensor_resize1d(state, colPtrs, inDim + 1);
+
+    // Prepare data for cusparse, need CudaInt buffers
+    THCTensor_(select)(state, sel, buf, 1, 0);
+    copyCudaFloatingType(state, rowInds, sel);
+    THCTensor_(select)(state, sel, buf, 1, 1);
+    copyCudaFloatingType(state, colbuf, sel);
+    THCTensor_(select)(state, sel, buf, 1, 2);
+    THCTensor_(copyCuda)(state, values, sel);
+
+    init_cusparse();
+    // Secretly coo2csc
+    cusparseXcoo2csr(cusparse_handle,
+                      THCudaIntTensor_data(state, colbuf), nnz, inDim,
+                      THCudaIntTensor_data(state, colPtrs), CUSPARSE_INDEX_BASE_ONE);
+
+    // FORTRAN expects contiguous col-major matrices
+    THCTensor *tgradOutput = THCTensor_(new)(state);
+    THCTensor_(transpose)(state, tgradOutput, gradOutput, 0, 1);
+    THCTensor_(resize2d)(state, buf, batchnum, outDim);
+    THCTensor_(copy)(state, buf, tgradOutput);
+    THCTensor_(free)(state, tgradOutput);
+
+    real one = ScalarConvert<int, real>::to(1);
+    cusparseMatDescr_t descr = 0;
+    cusparseCreateMatDescr(&descr);
+    cusparseSetMatType(descr, CUSPARSE_MATRIX_TYPE_GENERAL);
+    cusparseSetMatIndexBase(descr, CUSPARSE_INDEX_BASE_ONE);
+
+    cusparseSpMatDescr_t matA;
+    cusparseDnMatDescr_t matB, matC;
+
+    // Create sparse matrix A
+#ifdef THC_REAL_IS_FLOAT
+    cusparseCreateCsr(&matA, inDim, batchnum, nnz,
+                      THCudaIntTensor_data(state, rowInds),
+                      THCudaIntTensor_data(state, colbuf),
+                      THCTensor_(data)(state, values),
+                      CUSPARSE_INDEX_32I, CUSPARSE_INDEX_32I,
+                      CUSPARSE_INDEX_BASE_ONE, CUDA_R_32F);
+
+    // Create dense matrix B (gradOutput) and C (gradWeight)
+    cusparseCreateDnMat(&matB, batchnum, outDim, batchnum, buf, CUDA_R_32F, CUSPARSE_ORDER_COL);
+    cusparseCreateDnMat(&matC, inDim, outDim, inDim, gradWeight, CUDA_R_32F, CUSPARSE_ORDER_COL);
+
+    // Perform matrix multiplication (SpMM)
+    cusparseSpMM(cusparse_handle,
+                 CUSPARSE_OPERATION_NON_TRANSPOSE,
+                 CUSPARSE_OPERATION_NON_TRANSPOSE,
+                 &one,
+                 matA,
+                 matB,
+                 &one,
+                 matC,
+                 CUDA_R_32F,
+                 CUSPARSE_SPMM_ALG_DEFAULT,
+                 NULL); // Ensure dBuffer is allocated properly
+
+#elif defined(THC_REAL_IS_DOUBLE)
+    cusparseCreateCsr(&matA, inDim, batchnum, nnz,
+                      THCudaIntTensor_data(state, rowInds),
+                      THCudaIntTensor_data(state, colbuf),
+                      THCTensor_(data)(state, values),
+                      CUSPARSE_INDEX_32I, CUSPARSE_INDEX_32I,
+                      CUSPARSE_INDEX_BASE_ONE, CUDA_R_64F);
+
+    // Create dense matrix B (gradOutput) and C (gradWeight)
+    cusparseCreateDnMat(&matB, batchnum, outDim, batchnum, buf, CUDA_R_64F, CUSPARSE_ORDER_COL);
+    cusparseCreateDnMat(&matC, inDim, outDim, inDim, gradWeight, CUDA_R_64F, CUSPARSE_ORDER_COL);
+
+    // Perform matrix multiplication (SpMM)
+    cusparseSpMM(cusparse_handle,
+                 CUSPARSE_OPERATION_NON_TRANSPOSE,
+                 CUSPARSE_OPERATION_NON_TRANSPOSE,
+                 &one,
+                 matA,
+                 matB,
+                 &one,
+                 matC,
+                 CUDA_R_64F,
+                 CUSPARSE_SPMM_ALG_DEFAULT,
+                 NULL); // Ensure dBuffer is allocated properly
+#endif
+
+    // Sum gradOutput into gradBias
+    THCTensor_(sum)(state, buf, gradOutput, 0, 1);
+    THCTensor_(resize1d)(state, buf, outDim);
+    THCTensor_(cadd)(state, gradBias, gradBias, scale, buf);
+
+    // Apply weight decay if necessary
+    if (weightDecay != 0)
+    {
+        THCTensor_(cadd)(state, gradWeight, gradWeight, weightDecay, weight);
+        THCTensor_(cadd)(state, gradBias, gradBias, weightDecay, bias);
+    }
+
+    // Clean up
+    cusparseDestroySpMat(matA);
+    cusparseDestroyDnMat(matB);
+    cusparseDestroyDnMat(matC);
+    cusparseDestroyMatDescr(descr);
+    descr = 0;
+    THCTensor_(free)(state, weight);
+    THCTensor_(free)(state, buf);
+    THCTensor_(free)(state, sel);
+    THCTensor_(free)(state, cols);
+    THCudaLongTensor_free(state, inds);
+    THCTensor_(free)(state, values);
+    THCudaIntTensor_free(state, colbuf);
+    THCudaIntTensor_free(state, rowInds);
+    THCudaIntTensor_free(state, colPtrs);
 }
 
+
 void THNN_(SparseLinear_legacyUpdateOutput)(
            THCState *state,
            THCTensor *input,
